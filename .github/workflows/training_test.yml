# This workflow is used to publish a SageMaker Docker image containing with preinstalled TorchOk as
# well as to run SageMaker Training jobs to check for the quality degradation on defined training configurations
name: Training Test

on:
  pull_request:
    types:
      - closed
    branches:
      - main
  push:
    branches:
      - main
  release:
    types:
      - published

# Permissions settings used for AWS credentials
permissions:
  id-token: write
  contents: read
  packages: write

jobs:
  docker_build:
    if: github.event.pull_request.merged == true || github.event_name == 'push' || github.event_name == 'release'
    runs-on: ubuntu-latest
    # Repo checkout isn't needed here as build-push-action uses Git context
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE }}
          aws-region: eu-west-1

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
        with:
          registries: ${{ secrets.AWS_ACCOUNT }},763104351884

      # Buildx is used in favor of ordinal docker build in order to reuse build cache for future workflow runs
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Build and push latest version
        uses: docker/build-push-action@v4
        with:
          file: .ci/Dockerfile
          push: true
          tags: ${{ secrets.AWS_ACCOUNT }}.dkr.ecr.eu-west-1.amazonaws.com/dl-sm-torchok:latest
          cache-from: type=gha,mode=max
          cache-to: type=gha,mode=max

      - name: Build and push tagged version
        if: github.event.release.published == true
        uses: docker/build-push-action@v4
        with:
          file: .ci/Dockerfile
          push: true
          tags: ${{ secrets.AWS_ACCOUNT }}.dkr.ecr.eu-west-1.amazonaws.com/dl-sm-torchok:${{ github.ref_name }}
          cache-from: type=gha,mode=max
          cache-to: type=gha,mode=max

  # We want to run SageMaker training jobs only when the impactful files are changed
  check_modified_files:
    if: github.event.pull_request.merged == true || github.event_name == 'push'
    runs-on: ubuntu-latest
    outputs:
      any_changed: ${{ steps.changed-files.outputs.any_changed }}
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: Get changed files using defaults
        id: changed-files
        uses: tj-actions/changed-files@v35
        with:
          files: |
            pyproject.toml
            torchok
            .ci/Dockerfile
            tests/training_tests
      - name: Check what files were modified
        run: |
          for file in ${{ steps.changed-files.outputs.modified_files }}; do
            echo "$file was modified"
          done

  # Start SageMaker Training jobs for all training tests configurations in tests/training_tests/ directory
  start_sagemaker_jobs:
    if: |
      (github.event.pull_request.merged == true || github.event_name == 'push') && 
      needs.check_modified_files.outputs.any_changed == 'true'
    needs: [docker_build, check_modified_files]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE }}
          aws-region: eu-west-1
      - name: Run SageMaker training jobs
        run: |
          training_config_paths=`ls tests/training_tests/*.yaml`
          for file_path in $training_config_paths
          do
            config_name=$(basename $file_path .yaml)
            echo "Starting SageMaker Training job with config: $config_name"

            aws --region eu-west-1 sagemaker create-training-job \
            --training-job-name "torchok-ci-$(date +%m-%d-%Y-%H-%M-%S)" \
            --role-arn "${{ secrets.AWS_SAGEMAKER_ROLE }}" \
            --algorithm-specification '{
                "TrainingInputMode": "File",
                "TrainingImage": "${{ secrets.AWS_ACCOUNT }}.dkr.ecr.eu-west-1.amazonaws.com/dl-sm-torchok",
                "ContainerArguments": ["-cp", "/app/tests/training_tests", "-cn", "'${config_name}'"]}' \
            --input-data-config '[{
                "ChannelName":"train",
                "DataSource":{
                    "S3DataSource":{
                        "S3DataType":"S3Prefix",
                        "S3Uri":"${{ secrets.AWS_S3_TESTS_INPUT_PATH }}",
                        "S3DataDistributionType":"FullyReplicated"}}}]' \
            --output-data-config '{"S3OutputPath": "${{ secrets.AWS_S3_TESTS_OUTPUT_PATH }}"}' \
            --vpc-config '{
                "SecurityGroupIds": ["sg-0e6f494741bd91599"],
                "Subnets": ["subnet-0165cee0a076d3d8d"]}' \
            --environment '{
                "SM_CHANNEL_TRAINING": "/opt/ml/input/data/train",
                "CV_MLFLOW_TRACKING_URI": "${{ secrets.CV_MLFLOW_TRACKING_URI }}",
                "CV_S3_ARTIFACT_BUCKET": "${{ secrets.AWS_S3_TESTS_OUTPUT_PATH }}/mlflow"}' \
            --resource-config '{"VolumeSizeInGB":10,"InstanceCount":1,"InstanceType":"ml.g4dn.xlarge"}' \
            --stopping-condition '{"MaxRuntimeInSeconds": 86400}'
          done
